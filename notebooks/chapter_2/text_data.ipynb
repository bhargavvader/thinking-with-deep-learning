{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning and Organizing\n",
    "\n",
    "This jupyter notebook walks through the various parts of cleaning text data, as well as the various ways we can represent text data before feeding it into a variety of ML algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\"Apples taste like onions when we cannot smell.\",\n",
    "        \"I find it easy to compare apples and oranges.\",\n",
    "        \"Chopping onions can make you cry.\",\n",
    "        \"Peeling an orange does not make you cry.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['today']\n",
    "for stopword in my_stop_words:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, article = [], []\n",
    "for text in raw_texts:\n",
    "    doc = nlp(text)\n",
    "    article = []\n",
    "    for w in doc:\n",
    "        # if it's not a stop word or punctuation mark, add it to our article!\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            # we add the lematized version of the word\n",
    "            article.append(w.lemma_)\n",
    "\n",
    "    texts.append(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apples taste like onions when we cannot smell.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'taste', 'like', 'onion', 'smell']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'taste', 'like', 'onion', 'smell'],\n",
       " ['find', 'easy', 'compare', 'apple', 'orange'],\n",
       " ['chop', 'onion', 'cry'],\n",
       " ['peel', 'orange', 'cry']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(2, 1), (9, 1), (10, 1)],\n",
       " [(8, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_texts = tf_idf_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.2672612419124244),\n",
       " (1, 0.5345224838248488),\n",
       " (2, 0.2672612419124244),\n",
       " (3, 0.5345224838248488),\n",
       " (4, 0.5345224838248488)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.2672612419124244),\n",
       " (5, 0.5345224838248488),\n",
       " (6, 0.5345224838248488),\n",
       " (7, 0.5345224838248488),\n",
       " (8, 0.2672612419124244)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing with scikit-learn\n",
    "\n",
    "I prefer to use gensim and spaCy for pre-processing because it is more clear at each step what my data structure looks like. However, for an abstracted approach to text pre-processing, scikit-learn works well, especially when you want to feed it into a traditional machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(raw_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0]\n",
      " [0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(raw_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38274272 0.         0.         0.         0.         0.48546061\n",
      "  0.         0.38274272 0.         0.         0.         0.48546061\n",
      "  0.48546061]\n",
      " [0.41428875 0.         0.52547275 0.         0.52547275 0.\n",
      "  0.         0.         0.         0.52547275 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.66767854 0.         0.         0.         0.\n",
      "  0.52640543 0.52640543 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.52547275 0.         0.\n",
      "  0.41428875 0.         0.52547275 0.         0.52547275 0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low and High Dimensional Representations of Text and Documents\n",
    "\n",
    "Let us use a TruncatedSVD to perform a latent semantic analysis on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lsa.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80128627,  0.59828114],\n",
       "       [ 0.41506543,  0.90979156],\n",
       "       [ 0.93368336, -0.35809968],\n",
       "       [ 0.66405657, -0.74768233]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document here is represented by 2 features!\n",
    "We'll now have a quick peak at word and document embedding methods. We're going to use a bigger dataset, because these methods are only useful when we have a lot of data. Models based on very large datasets take a while to train, so we'll use a medium sized one which comes pre-loaded with gensim to illustare the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "text = open(lee_train_file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add some words to the stop word list\n",
    "texts, article = [], []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and w.text != 'I':\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.text == '\\n':\n",
    "        texts.append(article)\n",
    "        article = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel = Word2Vec(\n",
    "        texts,\n",
    "        size=100,\n",
    "        window=10,\n",
    "        workers=10,\n",
    "        iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('try', 0.9998903274536133),\n",
       " ('terrorism', 0.9998902082443237),\n",
       " ('organisation', 0.999885082244873),\n",
       " ('end', 0.9998667240142822),\n",
       " ('tell', 0.9998652935028076),\n",
       " (' ', 0.9998610019683838),\n",
       " ('administration', 0.9998598694801331),\n",
       " ('send', 0.9998557567596436),\n",
       " ('include', 0.9998554587364197),\n",
       " ('order', 0.9998529553413391)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar(\"war\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34525752, -0.12957555,  0.34972474,  0.4155247 , -0.18545586,\n",
       "       -0.26773235,  0.07060564, -0.10754981, -0.58271134, -0.02812324,\n",
       "        0.12701893, -0.22439618,  0.24774538, -0.30322817,  0.21102238,\n",
       "        0.1827755 ,  0.4506811 ,  0.2891772 ,  0.26695222,  0.0706595 ,\n",
       "       -0.38050628, -0.43690634, -0.10315417, -0.0245578 , -0.02507531,\n",
       "        0.04661441, -0.5475317 ,  0.18461464, -0.5660164 ,  0.2518374 ,\n",
       "        0.45197135, -0.19491771, -0.27495465,  0.3672681 , -0.15505324,\n",
       "       -0.1007716 ,  0.8724897 ,  0.10775444,  0.13028485,  0.46064293,\n",
       "        0.14717346,  0.18906482, -0.13330121, -0.18262424,  0.23302734,\n",
       "        0.54573405,  0.08639288,  0.52703696,  0.20400599,  0.05208682,\n",
       "       -0.49520203, -0.06883708,  0.04259687, -0.16549878, -0.7167367 ,\n",
       "        0.18014987,  0.7422534 , -0.13168082,  0.13075174, -0.26470852,\n",
       "        0.09263235, -0.23357728, -0.03404523,  0.63183576, -0.3100732 ,\n",
       "       -0.47226948, -0.57066786,  0.16443603, -0.35373497,  0.08247514,\n",
       "       -0.22810759, -0.22431962,  0.0648546 ,  0.526862  , -0.24042986,\n",
       "        0.17112136,  0.37803686, -0.7938539 ,  0.43378332, -0.41565776,\n",
       "       -0.30562407, -0.18918602,  0.1608461 , -0.15413983, -0.07224987,\n",
       "        0.81215066, -0.24058059,  0.04412535, -0.1553916 ,  0.40586695,\n",
       "       -0.45162994, -0.04151675, -0.1804858 ,  0.38043538, -0.3820153 ,\n",
       "       -0.51942694, -0.8002967 ,  0.43319884, -0.30512974, -0.54533446],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv[\"war\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_address = \"/Users/bhargavvader/open_source/comp-syn/GoogleNews-vectors-negative300.bin\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_w2v_model = gensim.models.KeyedVectors.load_word2vec_format(model_address, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_w2v_model.wv.most_similar(\"war\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, one model is trained on our tiny Lee News corpus, and the other is a much larger, Google News trained word2vec model which is easy to download off the net. We use pre-trained models when we think if the semantic meanings of words in our dataset would more or less match what the pre-trained model was trained on. Here, the lee corpus is indeed similar to the google news one, so we're good to go.\n",
    "\n",
    "So we see how we can represent words as vectors - how about documents? We can think of two methods, to start with - one would be to simply all the words in a document, and the other would be train a model which directly creates document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel = Doc2Vec(documents, vector_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below creates an average word vector or a document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(text, model, model_type=None):\n",
    "    if model_type == \"word2vec\":\n",
    "        vectors = []\n",
    "        for word in text:\n",
    "            try:\n",
    "                vectors.append(model.wv[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(vectors) > 0:\n",
    "            return np.mean(vectors, axis=0)\n",
    "    if model_type == \"doc2vec\":\n",
    "        vector = model.infer_vector(text)\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2682142 , -0.07888896,  0.26061997,  0.31413463, -0.11738992,\n",
       "       -0.20065062,  0.04619072, -0.0772755 , -0.42138678, -0.02469636,\n",
       "        0.09462594, -0.18488337,  0.2014185 , -0.22424342,  0.16963072,\n",
       "        0.14139858,  0.34091645,  0.22189157,  0.20122297,  0.05150986,\n",
       "       -0.27569416, -0.31368142, -0.08673512, -0.01736381, -0.00789848,\n",
       "        0.01510774, -0.41073263,  0.13885505, -0.42040724,  0.18828356,\n",
       "        0.33014837, -0.14593379, -0.18917058,  0.28934133, -0.11820923,\n",
       "       -0.06811216,  0.6573482 ,  0.07705043,  0.09628382,  0.36049658,\n",
       "        0.10771599,  0.15367366, -0.10155267, -0.12636974,  0.17510927,\n",
       "        0.41557407,  0.07295226,  0.3965811 ,  0.15871991,  0.03513982,\n",
       "       -0.37011492, -0.05729943,  0.04417052, -0.12004811, -0.5418533 ,\n",
       "        0.11611135,  0.57332295, -0.11296045,  0.10464352, -0.21007928,\n",
       "        0.07600577, -0.17845507, -0.03191077,  0.4627975 , -0.23279038,\n",
       "       -0.35830045, -0.43013048,  0.12390673, -0.2639923 ,  0.05655164,\n",
       "       -0.15875192, -0.16288799,  0.05112316,  0.39564365, -0.17151152,\n",
       "        0.12263124,  0.26904494, -0.5924355 ,  0.31583843, -0.31523484,\n",
       "       -0.22787035, -0.15679821,  0.11539169, -0.10700902, -0.05429995,\n",
       "        0.6035542 , -0.18196884,  0.03149303, -0.11577782,  0.31123307,\n",
       "       -0.32277912, -0.02808872, -0.12870562,  0.29183587, -0.28541207,\n",
       "       -0.39831465, -0.60781467,  0.3330296 , -0.2011683 , -0.42346993],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vector(texts[0], w2vmodel, model_type=\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14034925, -0.07355043,  0.17530759,  0.31008515, -0.20377183,\n",
       "       -0.04022722,  0.12827148, -0.05953085, -0.34053314,  0.12919652,\n",
       "        0.10260154, -0.14444058,  0.35538098, -0.11871804,  0.02506182,\n",
       "        0.34713542,  0.26218605,  0.2420148 ,  0.1434873 , -0.02697086,\n",
       "       -0.4388777 , -0.10003196, -0.13411474,  0.19283494,  0.1041904 ,\n",
       "       -0.05437026, -0.3918465 ,  0.07196473, -0.32920796,  0.3047832 ,\n",
       "        0.36662868, -0.09508774, -0.28502998,  0.05790954,  0.11061126,\n",
       "       -0.01562202,  0.5377991 , -0.05626499,  0.07402083,  0.5319284 ,\n",
       "        0.18036991, -0.05269691,  0.077445  , -0.30860364,  0.12932111,\n",
       "        0.29683298, -0.24227116,  0.17103316, -0.01458791, -0.10705908],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vector(texts[0], d2vmodel, model_type=\"doc2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go: those are ways we can represent documents in low dimensions (LSA), as a function of the words in the document (bag of words, TF-IDF), and using popular embedding methods such as word2vec and doc2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing for deep learning\n",
    "\n",
    "For deep learning algorithms, we use many of the methods we discussed earlier, such as tokenising and using a vocabulary to convert the words to word ids. Luckily for us, packages such as PyTorch and Keras include extensive pre-processing suites for their text based deep learning models.\n",
    "\n",
    "Specifically, torch uses torchtext for its text preprocessing. In the following lines of code we use torchtext.data.Field which is a base datatype for text pre-processing: we can tokenise, lowercase and pad our words. Most deep learning applications require to us to mention a sequence length which is constant - so sentences with fewer tokens need to be padded to make the data of uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len  =  15#@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhargavvader/open_source/thinking-with-deep-learning/venv/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(\n",
    "  tokenize    = spacy_tokenizer,\n",
    "  lower       = True,\n",
    "  batch_first = True,\n",
    "  init_token  = '<bos>',\n",
    "  eos_token   = '<eos>',\n",
    "  fix_length  = seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = [ 'The Brown Fox Jumped Over The Lazy Dog' ]\n",
    "minibatch = list(map(TEXT.preprocess, minibatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', 'the', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "minibatch = TEXT.pad(minibatch)\n",
    "print(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'<bos>': 1,\n",
       "         'the': 2,\n",
       "         'brown': 1,\n",
       "         'fox': 1,\n",
       "         'jumped': 1,\n",
       "         'over': 1,\n",
       "         'lazy': 1,\n",
       "         'dog': 1,\n",
       "         '<eos>': 1,\n",
       "         '<pad>': 5})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = functools.reduce(operator.concat, minibatch)\n",
    "counter = Counter(tokens)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TEXT.vocab_cls(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', '<bos>', '<eos>', 'brown', 'dog', 'fox', 'jumped', 'lazy', 'over']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x14eaab3d0>>, {'<unk>': 0, '<pad>': 1, 'the': 2, '<bos>': 3, '<eos>': 4, 'brown': 5, 'dog': 6, 'fox': 7, 'jumped': 8, 'lazy': 9, 'over': 10})\n"
     ]
    }
   ],
   "source": [
    "print(vocab.stoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  5,  7,  8, 10,  4,  9,  6,  3,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.numericalize(minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors we pass around as input data usually look like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "# test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "# tokenizer = get_tokenizer('basic_english')\n",
    "# vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "#                                       iter(io.open(train_filepath,\n",
    "#                                                    encoding=\"utf8\"))))\n",
    "\n",
    "# def data_process(raw_text_iter):\n",
    "#     data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "#                        dtype=torch.long) for item in raw_text_iter]\n",
    "#     return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
